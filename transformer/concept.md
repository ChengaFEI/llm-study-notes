# Transformer

## Evolution

1. Seq2Seq model
   1. Task: NMT (Neural Machine Translation).
   2. Architecture: RNN with LSTM and GRU modules.
   3. Problem: Forget far-away information in the long context.
2. Attention Mechanism
   1. Purpose: Capture long-range dependencies.
   2. Mechanism: Encode input information with attention weights to generate the context vector respectively.
