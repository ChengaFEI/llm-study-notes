# Red Teaming Language Models with Language Models

## Methods

1. Use an LM to generate adversarial test cases ("red-teaming")
   1. Benefits
      1. It is a automated, scalable, and efficient way to uncover a wide range of harmful outputs.
2. Test various methods, such as zero-shot, few-shot, and reinforcement learning, on 280B params model.

## Results

## Limitations

1. The bias inherent in the red-teaming LM might affect the diversity of test cases and the generalizability of the findings.
2. The red-teaming LM might not catch all possible test cases.
