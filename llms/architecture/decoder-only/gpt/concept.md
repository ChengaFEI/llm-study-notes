# GPT

## Definition

GPT - Generative Pre-trained Transformer.

## Model Architecture

A stack of Transformer blocks, where each block is a stack of a self-attention layer and a feed-forward layer.

## Training Objectives

Predict next token in a sequence.

## Context Direction

Unidirectional.

## Features

Although GPT is only trained with unsupervised learning, it proves to be useful in various NLP tasks such as translation, summarization, and Q&A.
