# Attention

## Self-Attention

- Features
  - Input sequence attends to the input sequence itself once.
- Applications
  - Normal Language Translation
  - Document Classification
  - Text Summarization
  - Sentence Embedding

## Multi-Head Attention

- Features
  - Input sequence attends to the input sequence itself multiple times.
- Applications
  - Complex Language Understanding (Syntax, Semantics, etc.)
  - Speech Recognition (Phonemes, Intonation, Context, etc.)
  - Recommendation System (Different user behaviors, etc.)
  - Graph Neural Network (Different node-to-node relationships, etc.)

## Causal Attention

- Features
  - Input sequence attends to the input sequence itself, but only to the left.
- Applications
  - Text Generation
  - Time-Series Prediction
  - Music Generation
  - Real-Time Translation

## Cross-Attention

- Features
  - Input sequence attends to the output sequence.
- Applications
  - Neural Machine Translation (Encoder-Decoder Architecture)
  - Image Captioning
  - Multi-Modal Tasks
